{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a simple Presentation Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial walks you through how to make your own **Presentation** module for the BCI.  By the end of this tutorial you will be able to:\n",
    " * design your own user interface screen with virtual 'buttons' selectable by brain signals\n",
    " * connect to the decoder and run through the calibration to train the BCI\n",
    " * use your designed user-interface screen to select buttons the on-screen buttons\n",
    "\n",
    "Before running this tutorial you should have read [how an evoked bci works](https://mindaffect-bci.readthedocs.io/en/latest/how_an_evoked_bci_works.html) to get an overview of how this BCI works and it's main components, and run through [quickstart tutorial](quickstart.ipynb) to quickly test your installation and try the BCI.\n",
    "\n",
    "* Note: The non-Jupyter Notebook version of this code and other presentation examples can be found in the* [examples/presentation directory](https://github.com/mindaffect/pymindaffectBCI/blob/doc/mindaffectBCI/examples/presentation/minimal_presentation.py)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presentation module is responsible for displaying the user-interface to the user with flickering options they can select from with brain signals (see [how an evoked bci works](https://mindaffect-bci.readthedocs.io/en/latest/how_an_evoked_bci_works.html) for more information).  In order for the presentation flickering to generate the strongest possible brain response, and hence maximise the BCI performance, it must display the correct stimuli to the user with precise timing and communicate this timing information to the MindAffect decoder.  Further, in order to train the decoding model presentation happens in two different modes: \n",
    "\n",
    "- _calibration_ mode where we cue the user where to attend to obtain correctly labelled brain data to train the machine learning algorithms in the decoder and\n",
    "- _prediction_ mode where the user actually uses the BCI to make selections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *noisetag* module provides a number of tools to hide this complexity (different modes, timestamp recording) from the application developers.  Using the most extreme of these all the application developer has to do is provide a function to _draw_ the display as instructed by the noisetag module. To use it, we import the module and create the noisetag object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindaffectBCI.noisetag import Noisetag, sumstats\n",
    "nt = Noisetag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Draw function\n",
    "We will now write a function to draw the screen. Here we use the python gaming library [pyglet](www.pyglet.org) to draw 2 squares on the screen, with the given colors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyglet\n",
    "# define a simple 2-squares drawing function\n",
    "def draw_squares(col1,col2):\n",
    "    # draw square 1: @100,190 , width=100, height=100\n",
    "    x=100; y=190; w=100; h=100;\n",
    "    pyglet.graphics.draw(4,pyglet.gl.GL_QUADS,\n",
    "                         ('v2f',(x,y,x+w,y,x+w,y+h,x,y+h)),\n",
    "             ('c3f',(col1)*4))\n",
    "    # draw square 2: @440,100\n",
    "    x=640-100-100\n",
    "    pyglet.graphics.draw(4,pyglet.gl.GL_QUADS,\n",
    "                         ('v2f',(x,y,x+w,y,x+w,y+h,x,y+h)),\n",
    "             ('c3f',(col2)*4))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the Display\n",
    "Now we write a function which, 1) asks the noisetag framework how the selectable squares should look, 2) updates the noisetag framework with information about how the display was updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary mapping from stimulus-state to colors\n",
    "state2color={0:(.2,.2,.2), # off=grey\n",
    "             1:(1,1,1),    # on=white\n",
    "             2:(0,1,0),    # cue=green\n",
    "             3:(0,0,1)}    # feedback=blue\n",
    "def draw(dt):\n",
    "    '''draw the display with colors from noisetag'''\n",
    "    # send info on the *previous* stimulus state, with the recorded vsync time (if available)\n",
    "    fliptime = window.lastfliptime if window.lastfliptime else nt.getTimeStamp()\n",
    "    nt.sendStimulusState(timestamp=fliptime)\n",
    "    # update and get the new stimulus state to display\n",
    "    try : \n",
    "        nt.updateStimulusState()\n",
    "        stimulus_state,target_state,objIDs,sendEvents=nt.getStimulusState()\n",
    "    except StopIteration :\n",
    "        pyglet.app.exit() # terminate app when noisetag is done\n",
    "        return\n",
    "\n",
    "    # draw the display with the instructed colors\n",
    "    if stimulus_state : \n",
    "        draw_squares(state2color[stimulus_state[0]],\n",
    "                     state2color[stimulus_state[1]])\n",
    "\n",
    "    # some textual logging of what's happening\n",
    "    if target_state is not None and target_state>=0:\n",
    "        print(\"*\" if target_state>0 else '.',end='',flush=True)\n",
    "    else:\n",
    "        print('.',end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step we integrate the behaviour of the output module created in the [Simple Output Tutorial](simple_output_tutorial.ipynb) by attaching a selection callback which will be called whenever a selection is made by the BCI. For now we will use the \"Hello World\" example created in the Output Tutorial and add a function that prints the object ID of our second square when selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helloworld(objID):\n",
    "    print(\"hello world\")\n",
    "\n",
    "def printID(objID):\n",
    "    print(\"Selected: %d\"%(objID))\n",
    "    \n",
    "# define a trivial selection handler    \n",
    "def selectionHandler(objID):\n",
    "    selection_mapping = {\n",
    "        1:helloworld,\n",
    "        2:printID\n",
    "    }\n",
    "    func = selection_mapping.get(objID)\n",
    "    func(objID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing Accuracy \n",
    "Now, we need a bit of python hacking. Because our BCI depends on accurate timelock of the brain data (EEG) with the visual display, we need to have accurate time-stamps for when the display changes. Fortunately, pyglet allows us to get this accuracy as it provides a flip method on windows which blocks until the display is actually updated. Thus we can use this to generate accurate time-stamps. We do this by adding a time-stamp recording function to the windows normal flip method with the following magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "def timedflip(self):\n",
    "    '''pseudo method type which records the timestamp for window flips'''\n",
    "    type(self).flip(self) # call the 'real' flip method...\n",
    "    self.lastfliptime=nt.getTimeStamp()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize the window to display the stimulus, and setup the flip-time recording for it.  Be sure that you have [vsync](https://en.wikipedia.org/wiki/Screen_tearing#Vertical_synchronization) turned-on. Many graphics cards turn this off by default, as it (in theory) gives higher frame rates for gaming. However, for our system, frame-rate is less important than exact timing, hence always turn vsync on for visual Brain-Compuber-Interfaces!  \n",
    "\n",
    "#### **Note:** always set ``fullscreen=True`` when using the presentation module to improve screen timing accuracy. We set it to ``False`` here so the tutorial stays visible when the stimulus is running.  \n",
    "\n",
    "#### **Note:** When running in a notebook the pyglet window always starts minimized -- so if you can't see it check your task bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the drawing window\n",
    "# make a default window, with fixed size for simplicty, and vsync for timing\n",
    "config = pyglet.gl.Config(double_buffer=True)\n",
    "window = pyglet.window.Window(fullscreen=False, width=640,height=480, vsync=True, config=config)\n",
    "\n",
    "# Setup the flip-time recording for this window\n",
    "window.flip = types.MethodType(timedflip,window)\n",
    "window.lastfliptime=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the BCI decoder in the background.\n",
    "To successfully test your presentation module it is important to have the other components of the BCI running. As explained in the [quickstart tutorial](quickstart.ipynb), additionally to the presentation we build here, we need the Hub, Decoder, and Acquisition components for a functioning BCI.  \n",
    "For a quick test (with fake data) of this presentation module you can run all these components with a given configuration file using.\n",
    "\n",
    "N.B. if you run directly in this notebook, *don't forget to shutdown the decoder* at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindaffectBCI.online_bci\n",
    "config = mindaffectBCI.online_bci.load_config('fake_recogniser.json')\n",
    "mindaffectBCI.online_bci.run(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can run this config from the command line with:\n",
    "\n",
    "```python3 -m mindaffectBCI.online_bci --config_file fake_recogniser.json``` \n",
    "\n",
    "Or from you Anaconda environment:   \n",
    "\n",
    "```python -m mindaffectBCI.online_bci --config_file fake_recognizer.json```\n",
    "\n",
    "**See our tutorial** [Running Custom Presentation](simple_integration_tutorial.ipynb) **to set-up a BCI using your own Presentation module**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Experiment!\n",
    "To run the experiment we connect to the Hub, add our selection handler, tell the noisetag module to run a complete BCI 'experiment' with calibration and feedback mode, and start the pyglet main loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the noise-tagging connection\n",
    "nt.connect(timeout_ms=5000)\n",
    "nt.addSelectionHandler(selectionHandler)\n",
    "\n",
    "# tell the noisetag framework to run a full : calibrate->prediction sequence\n",
    "nt.setnumActiveObjIDs(2)\n",
    "nt.startExpt(nCal=4,nPred=10,duration=4)\n",
    "\n",
    "# run the pyglet main loop\n",
    "pyglet.clock.schedule(draw)\n",
    "pyglet.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindaffectBCI.online_bci\n",
    "mindaffectBCI.online_bci.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
