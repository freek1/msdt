{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIY Brain Computer Interfacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is a deeper dive into the mindaffectBCI, which shows you how to manually start each of the components individually.  Before running this tutorial you should have read [how an evoked bci works](https://mindaffect-bci.readthedocs.io/en/latest/how_an_evoked_bci_works.html) to get an overview of how this BCI works, and run through [quickstart tutorial](quickstart.ipynb) to quickly test your installation and try the BCI.\n",
    "\n",
    "After following this tutorial, you will be able to;\n",
    " * configure the BCI startup to your needs, and \n",
    " * know how to replace any of the main components with your own version.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the mindaffectBCI decoder and other required modules. \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import mindaffectBCI.online_bci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Architecture\n",
    "\n",
    "The basic architecture of a mindaffectBCI is illustrated here.\n",
    "![SystemArchitecture.png](images/SystemArchitecture.png)\n",
    "\n",
    "To actually run the BCI we need to start each of these components:\n",
    "- UtopiaHub: This component is the central server which coordinates all the other pieces, and saves the data for offline analysis\n",
    "\n",
    "- Acquisition: This component talks to the *EEG Headset* and streams the data to the Hub\n",
    "\n",
    "- Decoder: This component analysis the EEG data to fit the subject specific model and generate predictions\n",
    "\n",
    "- Presentation: This component presents the User-Interface to the user, including any BCI specific stimuli which need to be presented. It also selects outputs when the BCI is sufficiently confident and generates the appropriate output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start the UtopiaHub by running the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------- HUB ------------------------------\n",
    "# start the utopia-hub process\n",
    "hub_process = mindaffectBCI.online_bci.startHubProcess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACQUISITION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hub is running we want to establish a connection between the amplifier and the hub to stream the EEG data.\n",
    "To achieve this the [Brainflow](https://brainflow.ai/) library is used. The brainflow driver has to be initialized with input parameters that depend on the amplifier in use: (Consult the [Brainflow docs](https://brainflow.readthedocs.io/en/stable/SupportedBoards.html) for a complete list of amplifiers supported by brainflow but currently untested with the MindAffect BCI.)\n",
    "\n",
    "|**Board** | ```board_id```|```serial_port```|```ip_address```|```ip_port```| \n",
    "|:-----------|:---------------|:-----------------|:----------------|:-------------|  \n",
    "|Ganglion  | 1             |dongle serial port(COM3, /dev/ttyUSB0...)| - | - |\n",
    "|Ganglion + WiFi Shield|4 |          -            |WIFI Shield IP(default 192.168.4.1)|any local port which is free|\n",
    "|Cyton     | 0             |dongle serial port(COM3, /dev/ttyUSB0...)| -| -|\n",
    "|Cyton + Wifi Shield|5|-|WIFI Shield IP(default 192.168.4.1)|any local port which is free|\n",
    "\n",
    "When using either the OpenBCI Ganglion or Cyton with an USB-dongle we have to pass the ```serial_port``` argument, to find the serial port in use by your amplifier follow the following instructions: \n",
    "\n",
    "### On Mac:\n",
    "1. Open a Terminal session\n",
    "2. Type: `ls /dev/cu.*`, and look for something like `/dev/cu.usbmodem1` (or similar):\n",
    "\n",
    "    ```\n",
    "    $ ls /dev/cu.*\n",
    "    /dev/cu.Bluetooth-Modem\t\t/dev/cu.iPhone-WirelessiAP\n",
    "    /dev/cu.Bluetooth-PDA-Sync\t/dev/cu.usbserial\n",
    "    /dev/cu.usbmodem1\n",
    "    ```\n",
    "\n",
    "    Then, `serial_port` should be defined as  `\"serial_port\":\"dev/cu.your_com_name\"`\n",
    "\n",
    "### On Windows:\n",
    "\n",
    "1. Open Device Manager and unfold Ports(COM&LPT), the com port number is shown behind your used bluetooth adapter. \n",
    "    ![comport.jpg](images/comport.jpg)\n",
    "    Then, `serial_port` should be defined as  `\"serial_port\":\"COM_X_\"`\n",
    "\n",
    "The code below shows the `acq_args` for the Ganglion, change the arguments for the board in use and run the code block to start the acquisition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the ganglion acquisition process\n",
    "# Using brainflow for the acquisition driver.  \n",
    "#  so change the board_id and other args to use other boards\n",
    "acq_args =dict(board_id=1, serial_port='com4') # connect to the ganglion\n",
    "acq_process = mindaffectBCI.online_bci.startacquisitionProcess('brainflow', acq_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N.B. Only use this cell if you just want to test with a *fake* eeg stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a fake-data stream\n",
    "# with 4-channels running at 200Hz\n",
    "acq_args=dict(host='localhost', nch=4, fs=200)\n",
    "acq_process = mindaffectBCI.online_bci.startacquisitionProcess('fakedata', acq_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECODER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is the core of the BCI as it takes in the raw EEG and stimulus information and generates predictions about which stimulus the user is attending to. Generating these predictions relies on signal processing and machine learning techniques to learn the best decoding parameters for each user. However, ensuring best performance means the settings for the decoder should be appropriate for the particular BCI being used. The default decoder parameters are shown in the code below, and are setup for a noisetagging BCI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the decoder process, wih default args for noise-tagging\n",
    "decoder_args = dict(\n",
    "        stopband=((45,65),(3,25,'bandpass')),  # frequency filter parameters\n",
    "        out_fs=100,  # sample rate after pre-processing\n",
    "        evtlabs=(\"re\",\"fe\"),  # use rising-edge and falling-edge as brain response triggers\n",
    "        tau_ms=450, # use 450ms as the brain response duration\n",
    "        calplots=True, # make the end-of-calibration model plots\n",
    "        predplots=False # don't make plots during prediction\n",
    "    )\n",
    "decoder_process = mindaffectBCI.online_bci.startDecoderProcess('decoder', decoder_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key parameters here are:\n",
    "\n",
    "- `stopband`: this is a [temporal filter](https://en.wikipedia.org/wiki/Filter_(signal_processing)) which is applied as a pre-processing step to the incoming  data.  This is important to remove external noise so the decoder can focus on the target brain signals.   \n",
    "\n",
    " Here the filter is specified as a list of [band stop](https://en.wikipedia.org/wiki/Band-stop_filter) filters, which specify which signal frequencies should be suppressed, (where, in classic python fashion -1 indicates the max-possible frequency).  Thus, in this example all frequencies below 3Hz and above 25Hz are removed.\n",
    "\n",
    "\n",
    "- `out_fs`: this specifies the post-filtering sampling rate of the data.  This reduces the amount of data which will be processed by the rest of the decoder.  Thus, in this example after filtering the data is re-sampled to 80Hz.  (Note: to avoid []() out_fs should be greater than 2x the maximum frequency passed by the stop-band).\n",
    "\n",
    "\n",
    "- `evtlabs`: this specifies the stimulus properties (or event labels) the decoder will try to predict from the brain responses.  The input to the decoder (and the brain) is the raw-stimulus intensity (i.e. it's brightness, or loudness).  However, depending on the task the user is performing, the brain may *not* respond directly to the brightness, but some other property of the stimulus.  \n",
    "\n",
    " For example, in the classic [P300 'odd-ball' BCI](https://en.wikipedia.org/wiki/P300_(neuroscience)#Applications), the brain responds not to the raw intensity, but to the start of *surprising* stimuli.  The design of the P300 matrix-speller BCI means this response happens when the user's chosen output 'flashes', or gets bright.  Thus, in the P300 BCI the brain responds to the [rising-edge](https://en.wikipedia.org/wiki/Signal_edge) of the stimulus intensity.   \n",
    "\n",
    " Knowing exactly what stimulus property the brain is responding to is a well studied neuroscientific research question, with examples including, stimulus-onset (a.k.a. rising-edge, or 're'), stimulus-offset (a.k.a. falling-edge, or 'fe'), stimulus intensity ('flash'), stimulus-duration etc.  Getting the right stimulus-coding is critical for BCI performance, see [`stim2event.py`](mindaffectBCI/decoder/stim2event.py) for more information on supported event types.\n",
    "\n",
    "\n",
    "- `tau_ms`: this specifies the maximum duration of the expected brain response to a triggering event in *milliseconds*.  As with the trigger type, the length of the brian response to a triggering event depends on the type of response expected.  For example for the P300 the response is between 300 and 600 ms after the trigger, whereas for a VEP the response is between 100 and 400 ms.   \n",
    "\n",
    " Ideally, the response window should be as small as possible, so the learning system only gets the brain response, and not a lot of non-response containing noise which could lead the machine learning component to [overfitt](https://en.wikipedia.org/wiki/Overfitting).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRESENTATION \n",
    "\n",
    "Before launching the presentation component we first make sure that the Hub, Acquisition, and Decoder components are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all is running?\n",
    "print(\"Hub running {}\".format(hub_process.poll() is None))\n",
    "print(\"Acquisition running {}\".format(acq_process.is_alive()))\n",
    "print(\"Decoder running {}\".format(decoder_process.is_alive()))\n",
    "print(\"Everything running? {}\".format(mindaffectBCI.online_bci.check_is_running(hub_process,acq_process,decoder_process)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not, try running the corresponding codeblock of the inactive component before continuing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Stimulus  \n",
    "By default we use the MindAffect NoiseTagging style stimulus with a 25-symbol letter matrix for presentation.  You can easily try different types of stimulus and selection matrices by modifying the `symbols` and `stimfile`arguments. Where:\n",
    "* _symbols_ : can either by a list-of-lists of the actual text to show, for example for a 2x2 grid of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " symbols=[[\"I'm happy\",\"I'm sad\"], [\"I want to play\",\"I want to sleep\"]],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or a file from which to load the set of symbols as a *comma-separated* list of strings like the file [symbols.txt](https://github.com/mindaffect/pymindaffectBCI/blob/open_source/mindaffectBCI/examples/presentation/symbols.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols=\"symbols.txt\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _stimfile_ : is a file which contains the stimulus-code to display.  This can either be a text-file with a matrix specified with a white-space separated line per output or a png with the stimulus with outputs in 'x' and time in 'y'. This is what the _codebook_ for the noisetag looks like where symbols are left to right and time is top to bottom.\n",
    "![stimfile.png](images/mgold_61_6521_psk_60hz.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Images as Stimuli \n",
    "We also support the use of images as stimuli as shown by the [robot_control.txt](https://github.com/mindaffect/pymindaffectBCI/blob/open_source/mindaffectBCI/examples/presentation/robot_control.txt) file. Simply specify the relative paths of the images you want to use as stimuli in a .txt file, or directly in a list-of-list as shown above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Stimulus\n",
    "The UI with the desired presentation stimuli can be launched by running the code below. \n",
    "### Note: the stimulus window my appear minimized, so check your task-bar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the presentation, with our matrix and default parameters for a noise tag\n",
    "from mindaffectBCI.examples.presentation import selectionMatrix\n",
    "selectionMatrix.run(symbols=symbols, stimfile=\"mgold_65_6532_psk_60hz.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the system is up and running, you can go through the following steps to use the BCI!\n",
    "\n",
    "1. EEG headset Setup\n",
    "\n",
    "    Prepare a headset such that it follows the [MindAffect headset layout.pdf](https://github.com/mindaffect/Headset/blob/master/MindAffect%20headset%20layout.pdf) in our Headset repository or prepare the headset delivered with your kit by following [MindAffect headset setup.pdf](https://github.com/mindaffect/Headset/raw/master/MindAffect%20Headset%20Set%20up%20instructions.pdf)\n",
    "\n",
    "\n",
    "2. Signal Quality\n",
    "\n",
    "    Check the signal quality by pressing 0 in the main menu. Try to adjust the headset until all electrodes are green, or noise to signal ratio is below 5. \n",
    "    You can try to improve the signal for an electrode by pressing it firmly into your head. After releasing pressure, wait a few seconds to see if the signal improves. If not, remove the electrode, and apply more water to the sponge. The sponges should feel wet on your scalp.\n",
    "    If  the noise to signal ratio does not improve by adjusting the headset, try to distance yourself from power outlets and other electronics.\n",
    "\n",
    "\n",
    "3. Calibration\n",
    "\n",
    "    Start calibration by pressing 1 in the main menu. Continue to follow the on-screen instructions.\n",
    "\n",
    "\n",
    "4. Feedback\n",
    "\n",
    "    You are now ready to try out the BCI by either selecting Copy-spelling (2) or Free-spelling (1)!\n",
    "    \n",
    "**Struggling to get the system to work? Consult our** [FAQ](https://mindaffect-bci.readthedocs.io/en/latest/FAQ.html) **section for info on how to improve calibration accuracy, prediction performance, and more!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHUTDOWN \n",
    "\n",
    "At this point, even though the presentation has completed, the background processes which run the hub, acquisation and decoder are still running.  To ensure they are stopped cleanly it is **always** a good idea to shut them down correctly, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shutdown the background processes\n",
    "# N.B. only needed if something went wrong..\n",
    "mindaffectBCI.online_bci.shutdown(hub_process, acq_process, decoder_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Run all components with 1 command\n",
    "If you just want to quickly run the decoding part of the BCI without presentation/output, using a pre-defined configuration you can do that easily by.\n",
    " 1. loading a configuration file\n",
    " 2. running the BCI\n",
    "This is demonstrated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = mindaffectBCI.online_bci.load_config('noisetag_bci')\n",
    "# uncomment this line to use fakedata\n",
    "#config['acquisition']='fakedata'\n",
    "mindaffectBCI.online_bci.run(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you not want to run this whole notebook everytime when using the BCI?\n",
    "\n",
    "Simply run it from your command prompt:    \n",
    "  \n",
    "``` python3 -m mindaffectBCI.online_bci```  \n",
    "   \n",
    "Or with a JSON configuration file:  \n",
    "  \n",
    "``` python3 -m mindaffectBCI.online_bci --config_file noisetag_bci.json ```   \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Further\n",
    "\n",
    " 1. Try other BCI types using our alternative configuration files.\n",
    "     * [noisetag.json](https://github.com/mindaffect/pymindaffectBCI/tree/open_source/mindaffectBCI/noisetag_bci.json) : example for a [noise-tagging](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0133797) (or c-VEP) BCI (Default)\n",
    "     * [rc5x5.json](https://github.com/mindaffect/pymindaffectBCI/tree/open_source/mindaffectBCI/rc5x5_bci.json) : example for a classic visual P300 odd-ball type BCI with row-column stimulus.\n",
    "     * [ssvep.json](https://github.com/mindaffect/pymindaffectBCI/tree/open_source/mindaffectBCI/ssvep_bci.json) : example for a classic [steady-state-visual-response](https://arxiv.org/abs/2002.01171) BCI.\n",
    "\n",
    "2. Write your own presentation system by following this guide (https://mindaffect-bci.readthedocs.io/en/latest/simple_presentation_tutorial.html)\n",
    "\n",
    "3. Write your own output system to make *interesting* things happen when a brain control is activated following this guide (https://mindaffect-bci.readthedocs.io/en/latest/simple_output_tutorial.html)\n",
    "\n",
    "4. Build your own BCI following this guide to develop your own compents. (https://mindaffect-bci.readthedocs.io/en/latest/first_run.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
